{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theophilus-Baidoo/Data--Driven-Survival-Modeling-for-Breast-Cancer/blob/main/Breast_CancerDeepSurv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f22533a",
      "metadata": {
        "id": "0f22533a"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn_pandas\n",
        "!pip install torchtuples\n",
        "!pip install pycox\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "665b7049",
      "metadata": {
        "id": "665b7049"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torchtuples as tt\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from pycox.models import CoxPH\n",
        "from pycox.models import DeepHitSingle\n",
        "from pycox.evaluation import EvalSurv\n",
        "from pycox.preprocessing.feature_transforms import OrderedCategoricalLong\n",
        "from pycox.datasets import support\n",
        "from pycox.models.loss import rank_loss_deephit_single\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import optuna\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchtuples as tt\n",
        "from pycox.models import CoxPH\n",
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64879588",
      "metadata": {
        "id": "64879588"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "mydata1 = pd.read_csv(\"C:/Users/gyedu/OneDrive/Desktop/SEER Breast Cancer Dataset .csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14afd702",
      "metadata": {
        "id": "14afd702"
      },
      "outputs": [],
      "source": [
        "# rename some of the column\n",
        "mydata1 = mydata1.rename(columns={'Race ': 'Race'})\n",
        "mydata1 = mydata1.rename(columns={'T Stage ': 'TStage'})\n",
        "mydata1= mydata1.rename(columns={'N Stage': 'NStage'})\n",
        "mydata1 = mydata1.rename(columns={'Marital Status': 'Marital'})\n",
        "mydata1= mydata1.rename(columns={'6th Stage': '6thStage'})\n",
        "mydata1 = mydata1.rename(columns={'A Stage': 'AStage'})\n",
        "mydata1 = mydata1.rename(columns={'Estrogen Status': 'EStatus'})\n",
        "mydata1 = mydata1.rename(columns={'Progesterone Status': 'PStatus'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a175ea",
      "metadata": {
        "id": "c2a175ea"
      },
      "outputs": [],
      "source": [
        "mydata1[\"Race\"].replace(\"Other (American Indian/AK Native, Asian/Pacific Islander)\", \"Other\", inplace=True)\n",
        "mydata1[\"Grade\"].replace(\"Well differentiated; Grade I\", \"Grade I\", inplace=True)\n",
        "mydata1[\"Grade\"].replace(\"Moderately differentiated; Grade II\", \"Grade II\", inplace=True)\n",
        "mydata1[\"Grade\"].replace(\"Poorly differentiated; Grade III\", \"Grade III\", inplace=True)\n",
        "mydata1[\"Grade\"].replace(\"Undifferentiated; anaplastic; Grade IV\", \"Grade IV\", inplace=True)\n",
        "mydata1[\"Marital\"].replace(\"Single (never married)\", \"Single\", inplace=True)\n",
        "mydata1[\"Marital\"].replace(\"Married (including common law)\", \"Married\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67efe0a3",
      "metadata": {
        "id": "67efe0a3"
      },
      "outputs": [],
      "source": [
        "mydata1['Status'].replace(['Alive','Dead'],[0,1],inplace=True)\n",
        "mydata1['TStage'].replace(['T1','T2','T3','T4'],[0,1,2,3],inplace=True)\n",
        "mydata1['NStage'].replace(['N1','N2','N3'],[0,1,2],inplace=True)\n",
        "mydata1['Grade'].replace(['Grade I','Grade II','Grade III','Grade IV'],[0,1,2,3],inplace=True)\n",
        "mydata1['EStatus'].replace(['Positive','Negative'],[1,0],inplace=True)\n",
        "mydata1['PStatus'].replace(['Positive','Negative'],[1,0],inplace=True)\n",
        "mydata1['AStage'].replace(['Regional','Distant'],[0,1],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "800e3974",
      "metadata": {
        "id": "800e3974"
      },
      "outputs": [],
      "source": [
        "mydata1 = mydata1.drop('6thStage',axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a32f3b",
      "metadata": {
        "id": "38a32f3b"
      },
      "outputs": [],
      "source": [
        "# Convert categorical variables into numerical ones using one-hot encoding\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "cat_cols = ['Race','Marital']\n",
        "encoded_cols = encoder.fit_transform(mydata1[cat_cols])\n",
        "encoded_cols_mydata1 = pd.DataFrame(encoded_cols.toarray(), columns=encoder.get_feature_names_out(cat_cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97eb42d3",
      "metadata": {
        "id": "97eb42d3"
      },
      "outputs": [],
      "source": [
        "mydata1= mydata1.drop(cat_cols, axis=1)\n",
        "mydata1 = pd.concat([mydata1, encoded_cols_mydata1], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7d4171c",
      "metadata": {
        "id": "d7d4171c"
      },
      "outputs": [],
      "source": [
        "mydata1_test=mydata1.sample(frac=0.2)\n",
        "mydata1_train=mydata1.drop(mydata1_test.index)\n",
        "mydata1_val=mydata1_train.sample(frac=0.2)\n",
        "mydata1_train=mydata1_train.drop(mydata1_val.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "126d54b2",
      "metadata": {
        "id": "126d54b2"
      },
      "outputs": [],
      "source": [
        "cols_stand = ['Age', 'Tumor Size', 'Regional Node Examined', 'Reginol Node Positive', 'TStage', 'NStage', 'Grade']\n",
        "\n",
        "cols_leave = ['EStatus','PStatus','AStage','Race_Black','Race_Other','Race_White', 'Marital_Divorced','Marital_Married','Marital_Separated',\n",
        "              'Marital_Single','Marital_Widowed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2223fc3",
      "metadata": {
        "id": "c2223fc3"
      },
      "outputs": [],
      "source": [
        "standardize = [([col], StandardScaler()) for col in cols_stand]\n",
        "leave = [(col, None) for col in cols_leave]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bc59e0f",
      "metadata": {
        "id": "3bc59e0f"
      },
      "outputs": [],
      "source": [
        "x_mapper = DataFrameMapper(standardize + leave)\n",
        "x_train = x_mapper.fit_transform(mydata1_train).astype('float32')\n",
        "x_val = x_mapper.transform(mydata1_val).astype('float32')\n",
        "x_test = x_mapper.transform(mydata1_test).astype('float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f5ed95",
      "metadata": {
        "id": "29f5ed95"
      },
      "outputs": [],
      "source": [
        "get_target = lambda mydata1: (mydata1['Survival Months'].values, mydata1['Status'].values)\n",
        "y_train = get_target(mydata1_train)\n",
        "y_val = get_target(mydata1_val)\n",
        "durations_test, events_test = get_target(mydata1_test)\n",
        "val = x_val, y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f9a653a",
      "metadata": {
        "collapsed": true,
        "id": "6f9a653a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def create_net(trial, input_features):\n",
        "    # Define hyperparameters\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 6)\n",
        "    units = trial.suggest_categorical('units', [32,64, 128])\n",
        "    dropout = trial.suggest_float('dropout',  0.1, 0.3)\n",
        "    #lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "\n",
        "    layers = []\n",
        "    for i in range(num_layers):\n",
        "        if i == 0:\n",
        "            layers.append(torch.nn.Linear(input_features, units))\n",
        "        else:\n",
        "            layers.append(torch.nn.Linear(units, units))\n",
        "        layers.append(torch.nn.ReLU())\n",
        "        layers.append(torch.nn.Dropout(dropout))\n",
        "\n",
        "    layers.append(torch.nn.Linear(units, 1))  # Output layer\n",
        "    return torch.nn.Sequential(*layers)\n",
        "\n",
        "# Then, use this function in your objective:\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
        "    net = create_net(trial, input_features=x_train.shape[1])\n",
        "    model = CoxPH(net, optimizer=tt.optim.Adam(lr=lr))\n",
        "\n",
        "    # Fit model\n",
        "    batch_size = trial.suggest_int('batch_size', 64, 100, log=True)\n",
        "    model.fit(x_train, y_train, batch_size, epochs=100, callbacks=[tt.callbacks.EarlyStopping()])\n",
        "\n",
        "    # Evaluate model\n",
        "    _ = model.compute_baseline_hazards()\n",
        "    # surv = model.predict_surv_df(x_test)\n",
        "    surv = model.predict_surv_df(x_val)\n",
        "    ev = EvalSurv(surv, y_val[0], y_val[1], censor_surv='km')\n",
        "    c_index = ev.concordance_td()\n",
        "\n",
        "    return -c_index  # Negative C-index because Optuna minimizes the objective\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "best_params = study.best_params\n",
        "print(\"Best parameters: \", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c9b94e",
      "metadata": {
        "collapsed": true,
        "id": "54c9b94e"
      },
      "outputs": [],
      "source": [
        "# best_params should contain keys like 'num_layers', 'units', 'dropout', etc.\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torchtuples as tt\n",
        "from pycox.models import CoxPH\n",
        "from pycox.evaluation import EvalSurv\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "c_indices_all = []\n",
        "ibs_scores_all = []\n",
        "\n",
        "for iteration in range(10):\n",
        "    random_state = 2345 + iteration\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "    c_indices = []\n",
        "    ibs_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
        "        x_train_fold = x_train[train_idx]\n",
        "        y_train_fold = y_train[0][train_idx], y_train[1][train_idx]\n",
        "        x_val_fold = x_train[val_idx]\n",
        "        y_val_fold = y_train[0][val_idx], y_train[1][val_idx]\n",
        "\n",
        "        # Using best parameters for MLPVanilla architecture\n",
        "        in_features = x_train.shape[1]\n",
        "        out_features = 1  # assuming CoxPH model\n",
        "        num_nodes = [best_params['units']] * best_params['num_layers']\n",
        "        batch_norm = False  # set as needed\n",
        "        dropout = best_params['dropout']\n",
        "        output_bias = False  # set as needed\n",
        "\n",
        "        net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout=dropout, output_bias=output_bias)\n",
        "        model = CoxPH(net, tt.optim.Adam(lr=best_params['lr']))\n",
        "\n",
        "        # Train the model\n",
        "        batch_size =90 #best_params['batch_size']\n",
        "        epochs = 100  # Adjust as necessary\n",
        "        callbacks = [tt.callbacks.EarlyStopping()]\n",
        "        verbose = True\n",
        "\n",
        "        log = model.fit(x_train_fold, y_train_fold, batch_size, epochs, callbacks, verbose,\n",
        "                        val_data=(x_val_fold, y_val_fold), val_batch_size=batch_size)\n",
        "#         log.plot()\n",
        "        # Evaluate the model\n",
        "        _ = model.compute_baseline_hazards()\n",
        "        surv = model.predict_surv_df(x_test)\n",
        "        # surv = model.predict_surv_df(x_val_fold)\n",
        "        # ev = EvalSurv(surv, y_val_fold[0], y_val_fold[1], censor_surv='km')\n",
        "        ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
        "\n",
        "        # Compute C-index and IBS\n",
        "        c_index = ev.concordance_td()\n",
        "        c_indices.append(c_index)\n",
        "#         time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
        "        time_grid = np.linspace(y_val_fold[0].min(), y_val_fold[0].max(), 100)\n",
        "        ibs = ev.integrated_brier_score(time_grid)\n",
        "        ibs_scores.append(ibs)\n",
        "\n",
        "    c_indices_all.append(np.mean(c_indices))\n",
        "    ibs_scores_all.append(np.mean(ibs_scores))\n",
        "\n",
        "    print(f'Iteration {iteration+1}, Average C-index: {np.mean(c_indices)}, Average IBS: {np.mean(ibs_scores)}')\n",
        "\n",
        "# Calculate the overall average and standard deviation outside the iterations loop\n",
        "final_avg_c_index = np.mean(c_indices_all)\n",
        "std_dev_c_index = np.std(c_indices_all)\n",
        "\n",
        "final_avg_ibs = np.mean(ibs_scores_all)\n",
        "std_dev_ibs = np.std(ibs_scores_all)\n",
        "\n",
        "print(f'Overall Average C-index: {final_avg_c_index}, Standard Deviation of C-index: {std_dev_c_index}')\n",
        "print(f'Overall Average IBS: {final_avg_ibs}, Standard Deviation of IBS: {std_dev_ibs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c6e366",
      "metadata": {
        "collapsed": true,
        "id": "31c6e366"
      },
      "outputs": [],
      "source": [
        "# best_params should contain keys like 'num_layers', 'units', 'dropout', etc.\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "import torchtuples as tt\n",
        "from pycox.models import CoxPH\n",
        "from pycox.evaluation import EvalSurv\n",
        "\n",
        "best_params = study.best_params\n",
        "\n",
        "c_indices_all = []\n",
        "ibs_scores_all = []\n",
        "\n",
        "for iteration in range(10):\n",
        "    random_state = 2775 + iteration\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "    c_indices = []\n",
        "    ibs_scores = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
        "        x_train_fold = x_train[train_idx]\n",
        "        y_train_fold = y_train[0][train_idx], y_train[1][train_idx]\n",
        "        x_val_fold = x_train[val_idx]\n",
        "        y_val_fold = y_train[0][val_idx], y_train[1][val_idx]\n",
        "\n",
        "        # Using best parameters for MLPVanilla architecture\n",
        "        in_features = x_train.shape[1]\n",
        "        out_features = 1  # assuming CoxPH model\n",
        "        num_nodes = [best_params['units']] * best_params['num_layers']\n",
        "        batch_norm = False  # set as needed\n",
        "        dropout = best_params['dropout']\n",
        "        output_bias = False  # set as needed\n",
        "\n",
        "        net = tt.practical.MLPVanilla(in_features, num_nodes, out_features, batch_norm, dropout=dropout, output_bias=output_bias)\n",
        "        model = CoxPH(net, tt.optim.Adam(lr=best_params['lr']))\n",
        "\n",
        "        # Train the model\n",
        "        batch_size =91 #best_params['batch_size']\n",
        "        epochs = 250  # Adjust as necessary\n",
        "        callbacks = [tt.callbacks.EarlyStopping()]\n",
        "        verbose = True\n",
        "\n",
        "        log = model.fit(x_train_fold, y_train_fold, batch_size, epochs, callbacks, verbose,\n",
        "                        val_data=(x_val_fold, y_val_fold), val_batch_size=batch_size)\n",
        "#         log.plot()\n",
        "        # Evaluate the model\n",
        "        _ = model.compute_baseline_hazards()\n",
        "        surv = model.predict_surv_df(x_test)\n",
        "        # surv = model.predict_surv_df(x_val_fold)\n",
        "        # ev = EvalSurv(surv, y_val_fold[0], y_val_fold[1], censor_surv='km')\n",
        "        ev = EvalSurv(surv, durations_test, events_test, censor_surv='km')\n",
        "\n",
        "        # Compute C-index and IBS\n",
        "        c_index = ev.concordance_td()\n",
        "        c_indices.append(c_index)\n",
        "#         time_grid = np.linspace(durations_test.min(), durations_test.max(), 100)\n",
        "        time_grid = np.linspace(y_val_fold[0].min(), y_val_fold[0].max(), 100)\n",
        "        ibs = ev.integrated_brier_score(time_grid)\n",
        "        ibs_scores.append(ibs)\n",
        "\n",
        "    c_indices_all.append(np.mean(c_indices))\n",
        "    ibs_scores_all.append(np.mean(ibs_scores))\n",
        "\n",
        "    print(f'Iteration {iteration+1}, Average C-index: {np.mean(c_indices)}, Average IBS: {np.mean(ibs_scores)}')\n",
        "\n",
        "# Calculate the overall average and standard deviation outside the iterations loop\n",
        "final_avg_c_index = np.mean(c_indices_all)\n",
        "std_dev_c_index = np.std(c_indices_all)\n",
        "\n",
        "final_avg_ibs = np.mean(ibs_scores_all)\n",
        "std_dev_ibs = np.std(ibs_scores_all)\n",
        "\n",
        "print(f'Overall Average C-index: {final_avg_c_index}, Standard Deviation of C-index: {std_dev_c_index}')\n",
        "print(f'Overall Average IBS: {final_avg_ibs}, Standard Deviation of IBS: {std_dev_ibs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c732cdfc",
      "metadata": {
        "collapsed": true,
        "id": "c732cdfc"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facb8f6f",
      "metadata": {
        "id": "facb8f6f"
      },
      "outputs": [],
      "source": [
        "# Convert your numpy arrays to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3696790",
      "metadata": {
        "id": "b3696790"
      },
      "outputs": [],
      "source": [
        "background = x_train_tensor  # You can choose a different subset\n",
        "\n",
        "# Initialize SHAP DeepExplainer\n",
        "explainer = shap.DeepExplainer(model.net, background)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb99db81",
      "metadata": {
        "collapsed": true,
        "id": "eb99db81"
      },
      "outputs": [],
      "source": [
        "# Compute SHAP values for a subset of your test set\n",
        "shap_values = explainer.shap_values(x_test_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d21e8e70",
      "metadata": {
        "collapsed": true,
        "id": "d21e8e70"
      },
      "outputs": [],
      "source": [
        "#  Visualization\n",
        "shap.summary_plot(shap_values, features=x_test_tensor.numpy(), feature_names=x_mapper.transformed_names_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d70c7d",
      "metadata": {
        "collapsed": true,
        "id": "e3d70c7d"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "import torch\n",
        "\n",
        "\n",
        "shap.summary_plot(shap_values, features=x_test, feature_names=x_mapper.transformed_names_, plot_type='bar')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}