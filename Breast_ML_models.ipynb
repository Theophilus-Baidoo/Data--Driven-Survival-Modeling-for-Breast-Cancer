{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theophilus-Baidoo/Data--Driven-Survival-Modeling-for-Breast-Cancer/blob/main/Breast_ML_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP-8Mvo3T1le"
      },
      "outputs": [],
      "source": [
        "\n",
        "import sys\n",
        "import warnings\n",
        "\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "######## Import all necessity functions for machine Learning ########\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif, mutual_info_regression\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, SVMSMOTE, BorderlineSMOTE, KMeansSMOTE, ADASYN\n",
        "from imblearn.ensemble import EasyEnsembleClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression, Perceptron, SGDClassifier, SGDRegressor\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, VotingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingClassifier\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report, silhouette_score\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "!pip install dask[dataframe]\n",
        "\n",
        "######### Import all necessary functions for Neural Network ##########\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, ReLU, LeakyReLU, PReLU, ELU, BatchNormalization, Dropout\n",
        "from tensorflow.keras.activations import relu, sigmoid, softmax, swish\n",
        "from tensorflow.keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform\n",
        "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, MSE, MAE, Huber\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Nadam, Adamax\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import L1, L2, L1L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUTZCysnCmdr"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk59z_gd93Up"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sipE1bXhUCiA"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SEER Breast Cancer Dataset .csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3a_asnLUFMg"
      },
      "outputs": [],
      "source": [
        "# rename the column 'Race ' to 'Race'\n",
        "df = df.rename(columns={'Race ': 'Race'})\n",
        "df = df.rename(columns={'T Stage ': 'TStage'})\n",
        "df = df.rename(columns={'N Stage': 'NStage'})\n",
        "df = df.rename(columns={'Marital Status': 'Marital'})\n",
        "df = df.rename(columns={'6th Stage': '6thStage'})\n",
        "df = df.rename(columns={'A Stage': 'AStage'})\n",
        "df = df.rename(columns={'Estrogen Status': 'EStatus'})\n",
        "df = df.rename(columns={'Progesterone Status': 'PStatus'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqgSCzM1UKGl"
      },
      "outputs": [],
      "source": [
        "df[\"Race\"].replace(\"Other (American Indian/AK Native, Asian/Pacific Islander)\", \"Other\", inplace=True)\n",
        "df[\"Grade\"].replace(\"Well differentiated; Grade I\", \"Grade I\", inplace=True)\n",
        "df[\"Grade\"].replace(\"Moderately differentiated; Grade II\", \"Grade II\", inplace=True)\n",
        "df[\"Grade\"].replace(\"Poorly differentiated; Grade III\", \"Grade III\", inplace=True)\n",
        "df[\"Grade\"].replace(\"Undifferentiated; anaplastic; Grade IV\", \"Grade IV\", inplace=True)\n",
        "df[\"Marital\"].replace(\"Single (never married)\", \"Single\", inplace=True)\n",
        "df[\"Marital\"].replace(\"Married (including common law)\", \"Married\", inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nii0lXDgUM0x"
      },
      "outputs": [],
      "source": [
        "df['Status'].replace(['Alive','Dead'],[0,1],inplace=True)\n",
        "df['TStage'].replace(['T1','T2','T3','T4'],[0,1,2,3],inplace=True)\n",
        "df['NStage'].replace(['N1','N2','N3'],[0,1,2],inplace=True)\n",
        "df['EStatus'].replace(['Positive','Negative'],[1,0],inplace=True)\n",
        "df['PStatus'].replace(['Positive','Negative'],[1,0],inplace=True)\n",
        "df['AStage'].replace(['Regional','Distant'],[0,1],inplace=True)\n",
        "df['Grade'].replace(['Grade I','Grade II','Grade III','Grade IV'],[0,1,2,3],inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f4yP0OWUS80"
      },
      "outputs": [],
      "source": [
        "# Convert categorical variables into numerical ones using one-hot encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')\n",
        "cat_cols = ['Race','Marital']\n",
        "encoded_cols = encoder.fit_transform(df[cat_cols])\n",
        "encoded_cols_df = pd.DataFrame(encoded_cols.toarray(), columns=encoder.get_feature_names_out(cat_cols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WOtYd-OUYWa"
      },
      "outputs": [],
      "source": [
        "df = df.drop((\"6thStage\"),axis=1)\n",
        "df = df.drop((\"Survival Months\"),axis=1)\n",
        "df = df.drop(cat_cols, axis=1)\n",
        "df = pd.concat([df, encoded_cols_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhpin9XgUbeb"
      },
      "outputs": [],
      "source": [
        "y=df.loc[:,'Status']\n",
        "df.drop(columns=['Status'],axis=1,inplace=True)\n",
        "X=df.iloc[:].values\n",
        "y=y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bELR15_kUotg"
      },
      "outputs": [],
      "source": [
        "!pip install imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmVefU1dhu78"
      },
      "outputs": [],
      "source": [
        "#pip install -U scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1Yq9AE2hu78"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from scipy.stats import randint\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Set random state\n",
        "random_state = 20\n",
        "\n",
        "# Ensure that X and y are defined and not None (replace 'X' and 'y' with your dataset variables)\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_state)\n",
        "\n",
        "# Print information about the dataset\n",
        "print(\"Total number of patients:\", X.shape[0])  # Updated from 'df.shape[0]' assuming X is your dataset\n",
        "print(\"Total number of patients in training set:\", X_train.shape[0])\n",
        "print(\"Total number of patients in test set:\", X_test.shape[0])\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=random_state)\n",
        "\n",
        "# Apply SMOTE on the training data\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train.astype('float'), y_train)\n",
        "\n",
        "# Display class distribution before and after SMOTE\n",
        "print('Before SMOTE:', Counter(y_train))\n",
        "print('After SMOTE:', Counter(y_train_smote))\n",
        "print('Test dataset shape:', Counter(y_test.values.ravel()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDH3CG4tVQPb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOUE9zaJkqWg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVdIjhLye06o"
      },
      "outputs": [],
      "source": [
        "# Lists to store evaluation metrics for each iteration\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "auc_scores = []\n",
        "confusion_matrices = []\n",
        "\n",
        "for i in range(10):\n",
        "    # SVM with a different random_state in each iteration\n",
        "    svm = SVC(kernel='rbf', probability=True, random_state=i)\n",
        "    param_grid = {'C': [0.1, 1, 10],\n",
        "                  'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n",
        "    grid_search = GridSearchCV(estimator=svm,\n",
        "                               param_grid=param_grid,\n",
        "                               scoring='accuracy', n_jobs=-1, cv=5, verbose=2)\n",
        "    grid_search.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    best_svc = grid_search.best_estimator_\n",
        "\n",
        "    # Fit and make predictions\n",
        "    best_svc.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    ypred1 = best_svc.predict(X_test)\n",
        "    y_pred_proba = best_svc.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate evaluation metrics and append to the respective lists\n",
        "    accuracy_scores.append(accuracy_score(y_test, ypred1))\n",
        "    precision_scores.append(precision_score(y_test, ypred1))\n",
        "    recall_scores.append(recall_score(y_test, ypred1))\n",
        "    f1_scores.append(f1_score(y_test, ypred1))\n",
        "    auc_scores.append(roc_auc_score(y_test, y_pred_proba))\n",
        "    confusion_matrices.append(confusion_matrix(y_test, ypred1))\n",
        "\n",
        "# Calculating mean and standard deviation for each metric\n",
        "mean_accuracy = np.mean(accuracy_scores)\n",
        "std_accuracy = np.std(accuracy_scores)\n",
        "mean_precision = np.mean(precision_scores)\n",
        "std_precision = np.std(precision_scores)\n",
        "mean_recall = np.mean(recall_scores)\n",
        "std_recall = np.std(recall_scores)\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "mean_auc = np.mean(auc_scores)\n",
        "std_auc = np.std(auc_scores)\n",
        "\n",
        "# Average confusion matrix\n",
        "average_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
        "\n",
        "# Output the results\n",
        "(mean_accuracy, std_accuracy, mean_precision, std_precision, mean_recall, std_recall, mean_f1, std_f1, mean_auc, std_auc, average_confusion_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0NCLrdPxszQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ABNs3nqdORS"
      },
      "outputs": [],
      "source": [
        "accuracy_scores1 = []\n",
        "precision_scores1 = []\n",
        "recall_scores1 = []\n",
        "f1_scores1 = []\n",
        "auc_scores1= []\n",
        "confusion_matrices1 = []\n",
        "\n",
        "for i in range(10):\n",
        "    rn_forest = RandomForestClassifier(n_jobs=-1, random_state=i)\n",
        "    param_grid = {\n",
        "        \"max_depth\": range(1, 10),\n",
        "        \"min_samples_leaf\": randint(25, 50),\n",
        "        \"min_samples_split\": range(50, 100, 2),\n",
        "        \"n_estimators\": range(1000, 8000, 500),\n",
        "        \"bootstrap\": [True]\n",
        "    }\n",
        "    forest_grid = RandomizedSearchCV(rn_forest, param_grid, cv=5)\n",
        "    forest_grid.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    best_forest = forest_grid.best_estimator_\n",
        "    best_forest.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    ypred3 = best_forest.predict(X_test)\n",
        "    y_pred_proba1 = best_forest.predict_proba(X_test)[:, 1]\n",
        "    accuracy_scores1.append(accuracy_score(y_test, ypred3))\n",
        "    precision_scores1.append(precision_score(y_test, ypred3))\n",
        "    recall_scores1.append(recall_score(y_test, ypred3))\n",
        "    f1_scores1.append(f1_score(y_test, ypred3))\n",
        "    auc_scores1.append(roc_auc_score(y_test, y_pred_proba1))\n",
        "    confusion_matrices1.append(confusion_matrix(y_test, ypred3))\n",
        "\n",
        "mean_accuracy1 = np.mean(accuracy_scores1)\n",
        "std_accuracy1 = np.std(accuracy_scores1)\n",
        "mean_precision1 = np.mean(precision_scores1)\n",
        "std_precision1 = np.std(precision_scores1)\n",
        "mean_recall1 = np.mean(recall_scores1)\n",
        "std_recall1 = np.std(recall_scores1)\n",
        "mean_f11 = np.mean(f1_scores1)\n",
        "std_f11 = np.std(f1_scores1)\n",
        "mean_auc1 = np.mean(auc_scores1)\n",
        "std_auc1 = np.std(auc_scores1)\n",
        "average_confusion_matrix1 = np.mean(confusion_matrices1, axis=0)\n",
        "\n",
        "(mean_accuracy1, std_accuracy1, mean_precision1, std_precision1, mean_recall1, std_recall1, mean_f11, std_f11, mean_auc1, std_auc1, average_confusion_matrix1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iPhC6PZz13V"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Lists to store evaluation metrics for each iteration\n",
        "accuracy_scores2 = []\n",
        "precision_scores2 = []\n",
        "recall_scores2 = []\n",
        "f1_scores2 = []\n",
        "auc_scores2 = []\n",
        "confusion_matrices2 = []\n",
        "\n",
        "for i in range(10):\n",
        "    # XGBoost with a different random_state in each iteration\n",
        "    xgb = XGBClassifier(n_jobs=-1, random_state=i)\n",
        "    param_grid = {\n",
        "        'max_depth': range(4, 40),\n",
        "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3, 0.6, 0.9, 0.95, 0.99],\n",
        "        'n_estimators': range(100, 1100, 100),\n",
        "        'min_child_weight': range(1, 11),\n",
        "        'subsample': np.arange(0.1, 1.1, 0.1),\n",
        "        'colsample_bytree': np.arange(0.1, 1.1, 0.1)\n",
        "    }\n",
        "    xgb_grid = RandomizedSearchCV(xgb, param_grid, cv=5)\n",
        "    xgb_grid.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    best_xgb = xgb_grid.best_estimator_\n",
        "\n",
        "    # Fit and make predictions\n",
        "    best_xgb.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    ypred5 = best_xgb.predict(X_test)\n",
        "    y_pred_proba2 = best_xgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate evaluation metrics and append to the respective lists\n",
        "    accuracy_scores2.append(accuracy_score(y_test, ypred5))\n",
        "    precision_scores2.append(precision_score(y_test, ypred5))\n",
        "    recall_scores2.append(recall_score(y_test, ypred5))\n",
        "    f1_scores2.append(f1_score(y_test, ypred5))\n",
        "    auc_scores2.append(roc_auc_score(y_test, y_pred_proba2))\n",
        "    confusion_matrices2.append(confusion_matrix(y_test, ypred5))\n",
        "\n",
        "# Calculating mean and standard deviation for each metric\n",
        "mean_accuracy2 = np.mean(accuracy_scores2)\n",
        "std_accuracy2 = np.std(accuracy_scores2)\n",
        "mean_precision2 = np.mean(precision_scores2)\n",
        "std_precision2 = np.std(precision_scores2)\n",
        "mean_recall2 = np.mean(recall_scores2)\n",
        "std_recall2 = np.std(recall_scores2)\n",
        "mean_f12 = np.mean(f1_scores2)\n",
        "std_f12 = np.std(f1_scores2)\n",
        "mean_auc2 = np.mean(auc_scores2)\n",
        "std_auc2 = np.std(auc_scores2)\n",
        "\n",
        "# Average confusion matrix\n",
        "average_confusion_matrix2 = np.mean(confusion_matrices2, axis=0)\n",
        "\n",
        "# Output the results\n",
        "(mean_accuracy2, std_accuracy2, mean_precision2, std_precision2, mean_recall2, std_recall2, mean_f12, std_f12, mean_auc2, std_auc2, average_confusion_matrix2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8F_AZuz3yzl"
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Lists to store evaluation metrics for each iteration\n",
        "accuracy_scores3 = []\n",
        "precision_scores3 = []\n",
        "recall_scores3 = []\n",
        "f1_scores3 = []\n",
        "auc_scores3 = []\n",
        "confusion_matrices3 = []\n",
        "\n",
        "for i in range(10):\n",
        "    # LGBM with a different random_state in each iteration\n",
        "    lgb = LGBMClassifier(objective='binary', n_jobs=-1, random_state=i)\n",
        "    param_grid = {\n",
        "        'max_depth': range(4, 40),\n",
        "        'num_leaves': range(20, 100),\n",
        "        'learning_rate': np.arange(0.1, 1.0, 0.1),\n",
        "        'n_estimators': range(100, 1100),\n",
        "        'min_child_samples': range(1, 72),\n",
        "        'subsample': np.arange(0.1, 1.1, 0.1),\n",
        "        'colsample_bytree': np.arange(0.1, 1.1, 0.1)\n",
        "    }\n",
        "    lgb_grid = RandomizedSearchCV(lgb, param_grid, cv=5)\n",
        "    lgb_grid.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    best_lgb = lgb_grid.best_estimator_\n",
        "\n",
        "    # Fit and make predictions\n",
        "    best_lgb.fit(X_train_smote, y_train_smote.values.ravel())\n",
        "    ypred6 = best_lgb.predict(X_test)\n",
        "    y_pred_proba3 = best_lgb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate evaluation metrics and append to the respective lists\n",
        "    accuracy_scores3.append(accuracy_score(y_test, ypred6))\n",
        "    precision_scores3.append(precision_score(y_test, ypred6))\n",
        "    recall_scores3.append(recall_score(y_test, ypred6))\n",
        "    f1_scores3.append(f1_score(y_test, ypred6))\n",
        "    auc_scores3.append(roc_auc_score(y_test, y_pred_proba3))\n",
        "    confusion_matrices3.append(confusion_matrix(y_test, ypred6))\n",
        "\n",
        "# Calculating mean and standard deviation for each metric\n",
        "mean_accuracy3 = np.mean(accuracy_scores3)\n",
        "std_accuracy3 = np.std(accuracy_scores3)\n",
        "mean_precision3 = np.mean(precision_scores3)\n",
        "std_precision3 = np.std(precision_scores3)\n",
        "mean_recall3 = np.mean(recall_scores3)\n",
        "std_recall3 = np.std(recall_scores3)\n",
        "mean_f13 = np.mean(f1_scores3)\n",
        "std_f13 = np.std(f1_scores3)\n",
        "mean_auc3 = np.mean(auc_scores3)\n",
        "std_auc3 = np.std(auc_scores3)\n",
        "\n",
        "# Average confusion matrix\n",
        "average_confusion_matrix3 = np.mean(confusion_matrices3, axis=0)\n",
        "\n",
        "# Output the results\n",
        "(mean_accuracy3, std_accuracy3, mean_precision3, std_precision3, mean_recall3, std_recall3, mean_f13, std_f13, mean_auc3, std_auc3, average_confusion_matrix3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfeff8KOEdVK"
      },
      "outputs": [],
      "source": [
        "# Plotting ROC Curves for SVM and Random Forest with mean AUC values\n",
        "\n",
        "# Recalculating the ROC Curve for SVM (using the last iteration's model)\n",
        "y_pred_proba_svm = best_svc.predict_proba(X_test)[:, 1]\n",
        "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_pred_proba_svm)\n",
        "auc_score_svm = roc_auc_score(y_test, y_pred_proba_svm)\n",
        "\n",
        "# Recalculating the ROC Curve for Random Forest (using the last iteration's model)\n",
        "y_pred_proba_rf = best_forest.predict_proba(X_test)[:, 1]\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)\n",
        "auc_score_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "#xgboost\n",
        "y_pred_proba_xgb = best_xgb.predict_proba(X_test)[:, 1]\n",
        "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)\n",
        "auc_score_xgb = roc_auc_score(y_test, y_pred_proba_xgb)\n",
        "\n",
        "#lgb\n",
        "y_pred_proba_lgb = best_lgb.predict_proba(X_test)[:, 1]\n",
        "fpr_lgb, tpr_lgb, _ = roc_curve(y_test, y_pred_proba_lgb)\n",
        "auc_score_lgb = roc_auc_score(y_test, y_pred_proba_lgb)\n",
        "\n",
        "# Plotting ROC curves\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_svm, tpr_svm, label=f'SVM (Mean AUC = {mean_auc:.2f})', color='blue')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'RF (Mean AUC = {mean_auc1:.2f})', color='green')\n",
        "plt.plot(fpr_xgb, tpr_xgb, label=f'XGB (Mean AUC = {mean_auc2:.2f})', color='red')\n",
        "plt.plot(fpr_lgb, tpr_lgb, label=f'LGB (Mean AUC = {mean_auc3:.2f})', color='yellow')\n",
        "plt.plot([0, 1], [0, 1], color='darkgrey', linestyle='--')\n",
        "\n",
        "plt.title('ROC Curve for ML models')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os4gJSn6RkSr"
      },
      "outputs": [],
      "source": [
        "!pip install shap\n",
        "!pip install transformers\n",
        "!pip install nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riIEb8gOCdhI"
      },
      "outputs": [],
      "source": [
        "#best_forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yseh_0R4SuCc"
      },
      "outputs": [],
      "source": [
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtZHW0ZOSRBT"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(best_forest.predict,X_test)\n",
        "shap_values = explainer(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENNBBB7iTA-U"
      },
      "outputs": [],
      "source": [
        "#shap.plots.beeswarm(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKeftWV3CvmZ"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values,feature_names=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "shap_values = shap.Explanation(values=shap_values.values, feature_names=df.columns)\n"
      ],
      "metadata": {
        "id": "0DHu9_cxNKNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDzmSwzrboCe"
      },
      "outputs": [],
      "source": [
        "shap.plots.bar(shap_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OYi2getTNY2"
      },
      "outputs": [],
      "source": [
        "explainer1 = shap.Explainer(best_xgb.predict,X_test)\n",
        "shap_values1 = explainer1(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgv58q4gG6kj"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values1,feature_names=df.columns)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}